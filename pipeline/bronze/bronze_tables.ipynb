{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d73fc5d",
   "metadata": {},
   "source": [
    "# Bronze Layer ETL Pipeline using Delta Live Tables\n",
    "\n",
    "This notebook implements the Bronze layer data ingestion from CSV files using Databricks Delta Live Tables (DLT). It processes 5 source files:\n",
    "\n",
    "1. customers.csv - Customer information\n",
    "2. transaction_btc.csv - Bitcoin transactions \n",
    "3. transaction_commodities.csv - Commodities transactions\n",
    "4. quotation_btc.csv - Bitcoin price quotations\n",
    "5. quotation_yfinance.csv - Gold price quotations\n",
    "\n",
    "The pipeline implements streaming ingestion with data quality checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e5040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define source paths - these will be passed as parameters to the pipeline\n",
    "SOURCE_DIR = \"dbfs:/FileStore/data_raw/\"\n",
    "\n",
    "# Define schema for each source\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"documento\", StringType(), True),\n",
    "    StructField(\"segmento\", StringType(), True),\n",
    "    StructField(\"pais\", StringType(), True),\n",
    "    StructField(\"estado\", StringType(), True),\n",
    "    StructField(\"cidade\", StringType(), True),\n",
    "    StructField(\"created_at\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "transaction_btc_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"data_hora\", TimestampType(), True),\n",
    "    StructField(\"ativo\", StringType(), True),\n",
    "    StructField(\"quantidade\", DoubleType(), True),\n",
    "    StructField(\"tipo_operacao\", StringType(), True),\n",
    "    StructField(\"moeda\", StringType(), True),\n",
    "    StructField(\"cliente_id\", StringType(), True),\n",
    "    StructField(\"canal\", StringType(), True),\n",
    "    StructField(\"mercado\", StringType(), True),\n",
    "    StructField(\"arquivo_origem\", StringType(), True),\n",
    "    StructField(\"importado_em\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "transaction_commodities_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"data_hora\", TimestampType(), True),\n",
    "    StructField(\"commodity_code\", StringType(), True),\n",
    "    StructField(\"quantidade\", DoubleType(), True),\n",
    "    StructField(\"tipo_operacao\", StringType(), True),\n",
    "    StructField(\"unidade\", StringType(), True),\n",
    "    StructField(\"moeda\", StringType(), True),\n",
    "    StructField(\"cliente_id\", StringType(), True),\n",
    "    StructField(\"canal\", StringType(), True),\n",
    "    StructField(\"mercado\", StringType(), True),\n",
    "    StructField(\"arquivo_origem\", StringType(), True),\n",
    "    StructField(\"importado_em\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "quotation_btc_schema = StructType([\n",
    "    StructField(\"ativo\", StringType(), True),\n",
    "    StructField(\"preco\", DoubleType(), True),\n",
    "    StructField(\"moeda\", StringType(), True),\n",
    "    StructField(\"horario_coleta\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "quotation_yfinance_schema = StructType([\n",
    "    StructField(\"ativo\", StringType(), True),\n",
    "    StructField(\"preco\", DoubleType(), True),\n",
    "    StructField(\"moeda\", StringType(), True),\n",
    "    StructField(\"horario_coleta\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b619099",
   "metadata": {},
   "source": [
    "## Bronze Tables Creation\n",
    "\n",
    "Here we create the bronze tables using DLT to ingest data from the source CSV files. We'll use the `@dlt.table` decorator with data quality expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527dcfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze table for customers\n",
    "@dlt.table(\n",
    "    name=\"bronze_customers\",\n",
    "    comment=\"Raw customer data ingested from CSV files\"\n",
    ")\n",
    "@dlt.expect_or_fail(\"valid_customer_id\", \"customer_id IS NOT NULL\")\n",
    "@dlt.expect_or_fail(\"valid_customer_documento\", \"documento IS NOT NULL\")\n",
    "@dlt.expect(\"valid_timestamp\", \"created_at IS NOT NULL\")\n",
    "def bronze_customers():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .schema(customers_schema)\n",
    "        .load(f\"{SOURCE_DIR}/customers.csv\")\n",
    "        .select(\"*\", current_timestamp().alias(\"_ingested_at\"))\n",
    "    )\n",
    "\n",
    "# Bronze table for Bitcoin transactions\n",
    "@dlt.table(\n",
    "    name=\"bronze_transactions_btc\",\n",
    "    comment=\"Raw Bitcoin transaction data ingested from CSV files\"\n",
    ")\n",
    "@dlt.expect_or_fail(\"valid_transaction_id\", \"transaction_id IS NOT NULL\")\n",
    "@dlt.expect_or_fail(\"valid_quantidade\", \"quantidade > 0\")\n",
    "@dlt.expect(\"valid_timestamp\", \"data_hora IS NOT NULL\")\n",
    "def bronze_transactions_btc():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .schema(transaction_btc_schema)\n",
    "        .load(f\"{SOURCE_DIR}/transacation_btc.csv\")\n",
    "        .select(\"*\", current_timestamp().alias(\"_ingested_at\"))\n",
    "    )\n",
    "\n",
    "# Bronze table for commodities transactions\n",
    "@dlt.table(\n",
    "    name=\"bronze_transactions_commodities\",\n",
    "    comment=\"Raw commodities transaction data ingested from CSV files\"\n",
    ")\n",
    "@dlt.expect_or_fail(\"valid_transaction_id\", \"transaction_id IS NOT NULL\")\n",
    "@dlt.expect_or_fail(\"valid_quantidade\", \"quantidade > 0\")\n",
    "@dlt.expect(\"valid_timestamp\", \"data_hora IS NOT NULL\")\n",
    "def bronze_transactions_commodities():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .schema(transaction_commodities_schema)\n",
    "        .load(f\"{SOURCE_DIR}/transaction_commodities.csv\")\n",
    "        .select(\"*\", current_timestamp().alias(\"_ingested_at\"))\n",
    "    )\n",
    "\n",
    "# Bronze table for Bitcoin quotations\n",
    "@dlt.table(\n",
    "    name=\"bronze_quotations_btc\",\n",
    "    comment=\"Raw Bitcoin price quotation data ingested from CSV files\"\n",
    ")\n",
    "@dlt.expect_or_fail(\"valid_ativo\", \"ativo IS NOT NULL\")\n",
    "@dlt.expect_or_fail(\"valid_preco\", \"preco > 0\")\n",
    "@dlt.expect(\"valid_timestamp\", \"horario_coleta IS NOT NULL\")\n",
    "def bronze_quotations_btc():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .schema(quotation_btc_schema)\n",
    "        .load(f\"{SOURCE_DIR}/quotation_btc.csv\")\n",
    "        .select(\"*\", current_timestamp().alias(\"_ingested_at\"))\n",
    "    )\n",
    "\n",
    "# Bronze table for YFinance quotations (Gold)\n",
    "@dlt.table(\n",
    "    name=\"bronze_quotations_yfinance\",\n",
    "    comment=\"Raw YFinance price quotation data ingested from CSV files\"\n",
    ")\n",
    "@dlt.expect_or_fail(\"valid_ativo\", \"ativo IS NOT NULL\")\n",
    "@dlt.expect_or_fail(\"valid_preco\", \"preco > 0\")\n",
    "@dlt.expect(\"valid_timestamp\", \"horario_coleta IS NOT NULL\")\n",
    "def bronze_quotations_yfinance():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .schema(quotation_yfinance_schema)\n",
    "        .load(f\"{SOURCE_DIR}/quotation_yfinance.csv\")\n",
    "        .select(\"*\", current_timestamp().alias(\"_ingested_at\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa72e9",
   "metadata": {},
   "source": [
    "## Pipeline Configuration and Deployment\n",
    "\n",
    "To deploy this pipeline:\n",
    "\n",
    "1. Create a Pipeline in the Databricks workspace\n",
    "2. Set the Pipeline Mode to \"Triggered\"\n",
    "3. Set the Source to this notebook\n",
    "4. Configure the following parameters:\n",
    "   - Target: The database where tables will be created (e.g., \"raw\")\n",
    "   - Storage Location: The DBFS path for table storage\n",
    "   - Source Directory: The path to source CSV files\n",
    "5. Set the Cluster Mode to \"Fixed Size\" with appropriate sizing\n",
    "6. Enable autoscaling if needed\n",
    "7. Click Create and then Start the pipeline\n",
    "\n",
    "Example configuration in JSON format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2452763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Pipeline Configuration\n",
    "{\n",
    "    \"name\": \"raw_to_bronze_etl\",\n",
    "    \"storage\": \"dbfs:/pipelines/raw_to_bronze\",\n",
    "    \"configuration\": {\n",
    "        \"pipelines.trigger.interval\": \"6 hour\",\n",
    "        \"source_path\": \"dbfs:/FileStore/data_raw/\",\n",
    "        \"target_database\": \"raw\"\n",
    "    },\n",
    "    \"clusters\": [\n",
    "        {\n",
    "            \"label\": \"default\",\n",
    "            \"autoscale\": {\n",
    "                \"min_workers\": 1,\n",
    "                \"max_workers\": 2\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"libraries\": [\n",
    "        {\n",
    "            \"notebook\": {\n",
    "                \"path\": \"/Repos/pipeline/bronze/bronze_tables\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"continuous\": false\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
